{
  "hash": "628f2fa2d33b7557c7e9c07cd7470beb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic regression\"\nsubtitle: \"Lecture 19\"\ndate: \"2024-11-07\"\nformat: \n  live-revealjs: \n    output-file: 19-logistic-regression-slides.html\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n## When last we left our heros...\n\nWe have been studying regression:\n\n-   What combinations of data types have we seen?\n\n-   What did the picture look like?\n\n## Recap: simple linear regression {.smaller}\n\nNumerical response and one numerical predictor:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/slr-1-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Recap: simple linear regression {.smaller}\n\nNumerical response and one categorical predictor (two levels):\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Recap: multiple linear regression {.smaller}\n\nNumerical response; numerical and categorical predictors:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Today: a *binary* response {.smaller}\n\n$$\ny = \n\\begin{cases}\n1 & &&\\text{eg. Yes, Win, True, Heads, Success}\\\\\n0 & &&\\text{eg. No, Lose, False, Tails, Failure}.\n\\end{cases}\n$$\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Who cares?\n\nIf we can model the relationship between predictors ($x$) and a binary response ($y$), we can use the model to do a special kind of prediction called *classification*.\n\n## Example: is the e-mail spam or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{word and character counts in an e-mail.}\n$$\n\n![](images/19/spam.png){fig-align=\"center\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's spam}\\\\\n0 & \\text{it's legit}\n\\end{cases}\n$$\n\nEthical concerns?\n\n## Example: is it cancer or not? {.smaller}\n\n$$\n\\mathbf{x}: \\text{features in a medical image.}\n$$\n\n![](images/19/head-neck.jpg){fig-align=\"center\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{it's cancer}\\\\\n0 & \\text{it's healthy}\n\\end{cases}\n$$\n\nEthical concerns?\n\n## Example: will they default? {.smaller}\n\n$$\n\\mathbf{x}: \\text{financial and demographic info about a loan applicant.}\n$$\n\n![](images/19/fico.jpg){fig-align=\"center\"}\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{applicant is at risk of defaulting on loan}\\\\\n0 & \\text{applicant is safe}\n\\end{cases}\n$$\n\nEthical concerns?\n\n## Example: will they re-offend? {.smaller}\n\n$$\n\\mathbf{x}: \\text{info about a criminal suspect and their case.}\n$$\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/19/machine-bias-petty-theft-1.png){fig-align=\"center\" width=\"300\" height=\"250\"}\n:::\n\n::: {.column width=\"50%\"}\n![](images/19/machine-bias-drug-posession-1.png){fig-align=\"center\" width=\"300\" height=\"250\"}\n:::\n:::\n\n$$\ny\n= \n\\begin{cases}\n1 & \\text{suspect is at risk of re-offending pre-trial}\\\\\n0 & \\text{suspect is safe}\n\\end{cases}\n$$\n\nEthical concerns?\n\n## How do we model this type of data?\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Straight line of best fit is a little silly\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Instead: S-curve of best fit {.smaller}\n\nInstead of modeling $y$ directly, we model the probability that $y=1$:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n-   \"Given new email, what's the probability that it's spam?''\n-   \"Given new image, what's the probability that it's cancer?''\n-   \"Given new loan application, what's the probability that they default?''\n\n## Why don't we model y directly?\n\n-   **Recall regression with a numerical response**:\n\n    -   Our models do not output *guarantees* for $y$, they output predictions that describe behavior *on average*;\n\n-   **Similar when modeling a binary response**:\n\n    -   Our models cannot directly guarantee that $y$ will be zero or one. The correct analog to \"on average\" for a 0/1 response is \"what's the probability?\"\n\n## So, what is this S-curve, anyway?\n\nIt's the *logistic function*:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x}}{1+e^{\\beta_0+\\beta_1x}}.\n$$\n\nIf you set p = Prob(y = 1) and do some algebra, you get the simple linear model for the *log-odds*:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\nThis is called the *logistic regression* model.\n\n## Log-odds?\n\n-   p = Prob(y = 1) is a probability.\n    A number between 0 and 1;\n\n-   p / (1 - p) is the odds.\n    A number between 0 and $\\infty$;\n\n\"The odds of this lecture going well are 10 to 1.\"\n\n-   The log odds log(p / (1 - p)) is a number between $-\\infty$ and $\\infty$, which is suitable for the linear model.\n\n## Logistic regression\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x.\n$$\n\n-   The *logit* function log(p / (1-p)) is an example of a *link function* that transforms the linear model to have an appropriate range;\n\n-   This is an example of a *generalized linear model*;\n\n## Estimation\n\n-   We estimate the parameters $\\beta_0,\\,\\beta_1$ using *maximum likelihood* (don't worry about it) to get the \"best fitting\" S-curve;\n\n-   The fitted model is\n\n$$\n\\log\\left(\\frac{\\widehat{p}}{1-\\widehat{p}}\\right)\n=\nb_0+b_1x.\n$$\n\n## Logistic regression -\\> classification?\n\n## Step 1: pick a threshold {.smaller}\n\nSelect a number $0 < p^* < 1$:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/step-1-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$;\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 2: find the \"decision boundary\" {.smaller}\n\nSolve for the x-value that matches the threshold:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/step-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n-   if $\\text{Prob}(y=1)\\leq p^*$, then predict $\\widehat{y}=0$;\n-   if $\\text{Prob}(y=1)> p^*$, then predict $\\widehat{y}=1$.\n\n## Step 3: classify a new arrival {.smaller}\n\nA new person shows up with $x_{\\text{new}}$.\nWhich side of the boundary are they on?\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/step-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Let's change the threshold {.smaller}\n\nA new person shows up with $x_{\\text{new}}$.\nWhich side of the boundary are they on?\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/lower-threshold-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n-   if $x_{\\text{new}} \\leq x^\\star$, then $\\text{Prob}(y=1)\\leq p^*$, so predict $\\widehat{y}=0$ for the new person;\n-   if $x_{\\text{new}} > x^\\star$, then $\\text{Prob}(y=1)> p^*$, so predict $\\widehat{y}=1$ for the new person.\n\n## Nothing special about one predictor... {.smaller}\n\nTwo numerical predictors and one binary response:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## \"Multiple\" logistic regression\n\nOn the probability scale:\n\n$$\n\\text{Prob}(y = 1)\n=\n\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m}}.\n$$\n\nFor the log-odds, a *multiple* linear regression:\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right)\n=\n\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_mx_m.\n$$\n\n## Decision boundary, again {.smaller}\n\nIt's linear!\nConsider two numerical predictors:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](19-logistic-regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n-   if new $(x_1,\\,x_2)$ below, $\\text{Prob}(y=1)\\leq p^*$. Predict $\\widehat{y}=0$ for the new person;\n-   if new $(x_1,\\,x_2)$ above, $\\text{Prob}(y=1)> p^*$. Predict $\\widehat{y}=1$ for the new person.\n",
    "supporting": [
      "19-logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}