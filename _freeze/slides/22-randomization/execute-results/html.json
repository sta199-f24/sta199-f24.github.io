{
  "hash": "391fb63583ffde951e00a3c01fd40208",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Making decisions with randomization tests\"\nsubtitle: \"Lecture 22\"\ndate: \"2024-11-19\"\nformat: \n  live-revealjs: \n    output-file: 22-randomization-slides.html\nwebr:\n  cell-options:\n    autorun: false\n---\n\n\n\n\n\n\n\n\n\n<!-- begin: webr fodder -->\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| echo: false\n#| output: false\noptions(width = 60)\n```\n:::\n\n\n\n\n\n\n\n\n\n<!-- end: webr fodder -->\n\n<!-- begin: ae definition -->\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<!-- end: ae definition -->\n\n# Warm-up\n\n## While you wait... {.smaller}\n\n::: appex\n-   Go to your `ae` project in RStudio.\n\n-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   Click Pull to get today's application exercise file: *ae\\-18\\-duke\\-forest\\-bootstrap\\.qmd*.\n\n-   Wait till the you're prompted to work on the application exercise during class before editing the file.\n:::\n\n## Announcements {.xxsmall}\n\n**Before Monday, Nov 25**\n\n-   Bare minimum: Make substantial progress on your project write-up: Finish your introduction and exploratory data analysis (plots/summary statistics + their interpretations) + Write up methods you plan to use.\n-   Ideal: Start implementing the methods and get closer to answering your research question.\n-   Your work goes in `index.qmd` -- as of yesterday 50% of teams had not yet touched this file!\n-   View (and review) your rendered project on your project website linked from the \"About\" section of your project repo.\n\n# From last time: Quantifying uncertainty\n\n## Packages\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\n```\n:::\n\n\n\n\n\n\n\n\n\n## Data: Houses in Duke Forest {.smaller}\n\n::::: columns\n::: {.column width=\"50%\"}\n-   Data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\n-   Scraped from Zillow\n-   Source: [`openintro::duke_forest`](http://openintrostat.github.io/openintro/reference/duke_forest.html)\n:::\n\n::: {.column width=\"50%\"}\n![](images/21/duke_forest_home.jpg){fig-alt=\"Home in Duke Forest\"}\n:::\n:::::\n\n**Goal**: Use the area (in square feet) to understand variability in the price of houses in Duke Forest.\n\n## Modeling {.smaller}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_fit <- linear_reg() |>\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |>\n  kable(digits = 2) # neatly format table to 2 digits\n```\n\n::: {.cell-output-display}\n\n\n|term        |  estimate| std.error| statistic| p.value|\n|:-----------|---------:|---------:|---------:|-------:|\n|(Intercept) | 116652.33|  53302.46|      2.19|    0.03|\n|area        |    159.48|     18.17|      8.78|    0.00|\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n## Confidence interval for the slope {.smaller}\n\nA confidence interval will allow us to make a statement like \"*For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus X dollars.*\"\n\n## Slopes of bootstrap samples {.smaller}\n\n::: task\n**Fill in the blank:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus \\_\\_\\_ dollars.\n:::\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22-randomization_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Slopes of bootstrap samples {.smaller}\n\n::: task\n**Fill in the blank:** For each additional square foot, we expect the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus \\_\\_\\_ dollars.\n:::\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22-randomization_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Confidence level {.smaller}\n\n::: task\nHow confident are you that the true slope is between \\$0 and \\$250?\nHow about \\$150 and \\$170?\nHow about \\$90 and \\$210?\n:::\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22-randomization_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## 95% confidence interval {.smaller}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](22-randomization_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n::: incremental\n-   A 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\n-   We are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77.\n:::\n\n## ae\\-18\\-duke\\-forest\\-bootstrap {.smaller}\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-18\\-duke\\-forest\\-bootstrap\\.qmd*.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n\n## Computing the CI for the slope I\n\nCalculate the observed slope:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_fit <- duke_forest |>\n  specify(price ~ area) |>\n  fit()\n\nobserved_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept  116652.\n2 area          159.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Computing the CI for the slope II {.smaller}\n\nTake `100` bootstrap samples and fit models to each one:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"1,5,6\"}\nset.seed(1120)\n\nboot_fits <- duke_forest |>\n  specify(price ~ area) |>\n  generate(reps = 100, type = \"bootstrap\") |>\n  fit()\n\nboot_fits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       <int> <chr>        <dbl>\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# ℹ 190 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Computing the CI for the slope III\n\n**Percentile method:** Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"5\"}\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\" # default method\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          92.1     223.\n2 intercept -36765.   296528.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Precision vs. accuracy {.smaller}\n\n::: task\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval?\nWhat drawbacks are associated with using a wider interval?\n:::\n\n. . .\n\n![](images/21/garfield.png)\n\n## Precision vs. accuracy {.smaller}\n\n::: task\nHow can we get best of both worlds -- high precision and high accuracy?\n:::\n\n## Changing confidence level {.smaller}\n\n::: task\nHow would you modify the following code to calculate a 90% confidence interval?\nHow would you modify it for a 99% confidence interval?\n:::\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|4\"}\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          92.1     223.\n2 intercept -36765.   296528.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Changing confidence level {.smaller}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          104.     212.\n2 intercept  -24380.  256730.\n```\n\n\n:::\n\n```{.r .cell-code}\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  <chr>        <dbl>    <dbl>\n1 area          56.3     226.\n2 intercept -61950.   370395.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Recap {.xsmall}\n\n-   **Population:** Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = $N$)\n\n-   **Sample:** Subset of the population, ideally random and representative (sample size = $n$)\n\n-   Sample statistic $\\ne$ population parameter, but if the sample is good, it can be a good estimate\n\n-   **Statistical inference:** Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\n\n-   We report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\n\n-   Since we can't continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability\n\n## Why do we construct confidence intervals?\n\nTo estimate plausible values of a parameter of interest, e.g., a slope ($\\beta_1$), a mean ($\\mu$), a proportion ($p$).\n\n## What is bootstrapping?\n\n-   Bootstrapping is a statistical procedure that resamples(with replacement) a single data set to create many simulated samples.\n\n-   We then use these simulated samples to quantify the uncertainty around the sample statistic we're interested in, e.g., a slope ($b_1$), a mean ($\\bar{x}$), a proportion ($\\hat{p}$).\n\n## What does each dot on the plot represent? {.smaller}\n\nNote: The plot is of a bootstrap distribution of a sample mean.\n\n-   Resample, with replacement, from the original data\n-   Do this 20 times (since there are 20 dots on the plot)\n-   Calculate the summary statistic of interest in each of these samples\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n## Bootstrapping for categorical data\n\n-   `specify(response = x, success = \"success level\")`\n\n-   `calculate(stat = \"prop\")`\n\n## Bootstrapping for other `stat`s\n\n-   `calculate()` documentation: [infer.tidymodels.org/reference/calculate.html](https://infer.tidymodels.org/reference/calculate.html)\n\n-   **infer** pipelines: [infer.tidymodels.org/articles/observed_stat_examples.html](https://infer.tidymodels.org/articles/observed_stat_examples.html)\n\n# Hypothesis testing\n\n## Hypothesis testing\n\nA hypothesis test is a statistical technique used to evaluate *competing claims* using data\n\n::: incremental\n-   **Null hypothesis,** $H_0$: An assumption about the population.\n    \"There is nothing going on.\"\n\n-   **Alternative hypothesis,** $H_A$: A research question about the population.\n    \"There is something going on\".\n:::\n\n. . .\n\nNote: Hypotheses are always at the population level!\n\n## Setting hypotheses\n\n-   **Null hypothesis,** $H_0$: \"There is nothing going on.\" The slope of the model for predicting the prices of houses in Duke Forest from their areas is 0, $\\beta_1 = 0$.\n\n-   **Alternative hypothesis,** $H_A$: \"There is something going on\".\n    The slope of the model for predicting the prices of houses in Duke Forest from their areas is different than, $\\beta_1 \\ne 0$.\n\n## Hypothesis testing \"mindset\"\n\n-   Assume you live in a world where null hypothesis is true: $\\beta_1 = 0$.\n\n-   Ask yourself how likely you are to observe the sample statistic, or something even more extreme, in this world: $P(b_1 \\leq 159.48~or~b_1 \\geq 159.48 | \\beta_1 = 0)$ = ?\n\n## Hypothesis testing as a court trial {.smaller}\n\n-   **Null hypothesis**, $H_0$: Defendant is innocent\n\n-   **Alternative hypothesis**, $H_A$: Defendant is guilty\n\n. . .\n\n-   **Present the evidence:** Collect data\n\n. . .\n\n-   **Judge the evidence:** \"Could these data plausibly have happened by chance if the null hypothesis were true?\"\n    -   Yes: Fail to reject $H_0$\n    -   No: Reject $H_0$\n\n## Hypothesis testing framework {.smaller}\n\n::: incremental\n-   Start with a null hypothesis, $H_0$, that represents the status quo\n\n-   Set an alternative hypothesis, $H_A$, that represents the research question, i.e. what we’re testing for\n\n-   Conduct a hypothesis test under the assumption that the null hypothesis is true and calculate a **p-value** (probability of observed or more extreme outcome given that the null hypothesis is true)\n\n    -   if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\n    -   if they do, then reject the null hypothesis in favor of the alternative\n:::\n\n## Calculate observed slope\n\n...\nwhich we have already done:\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobserved_fit <- duke_forest |>\n  specify(price ~ area) |>\n  fit()\n\nobserved_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept  116652.\n2 area          159.\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Simulate null distribution\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1|2|3|4|5|6\"}\nset.seed(20241118)\nnull_dist <- duke_forest |>\n  specify(price ~ area) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 100, type = \"permute\") |>\n  fit()\n```\n:::\n\n\n\n\n\n\n\n\n\n## View null distribution {.smaller}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term        estimate\n       <int> <chr>          <dbl>\n 1         1 intercept 547294.   \n 2         1 area           4.54 \n 3         2 intercept 568599.   \n 4         2 area          -3.13 \n 5         3 intercept 561547.   \n 6         3 area          -0.593\n 7         4 intercept 526286.   \n 8         4 area          12.1  \n 9         5 intercept 651476.   \n10         5 area         -33.0  \n# ℹ 190 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Visualize null distribution {.smaller}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist |>\n  filter(term == \"area\") |>\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 15)\n```\n\n::: {.cell-output-display}\n![](22-randomization_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Visualize null distribution (alternative)\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisualize(null_dist) +\n  shade_p_value(obs_stat = observed_fit, direction = \"two-sided\")\n```\n\n::: {.cell-output-display}\n![](22-randomization_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Get p-value\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist |>\n  get_p_value(obs_stat = observed_fit, direction = \"two-sided\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the\n`generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\nPlease be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the\n`generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term      p_value\n  <chr>       <dbl>\n1 area            0\n2 intercept       0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n## Make a decision\n\n::: task\nBased on the p-value calculated, what is the conclusion of the hypothesis test?\n:::\n",
    "supporting": [
      "22-randomization_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}