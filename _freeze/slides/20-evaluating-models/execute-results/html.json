{
  "hash": "08da5cbd694a473c3b0dbf1ef5222cce",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Evaluating models\"\nsubtitle: \"Lecture 20\"\ndate: \"2024-11-12\"\nformat: \n  live-revealjs: \n    output-file: 20-evaluating-models-slides.html\nwebr:\n  cell-options:\n    autorun: false\n  packages:\n    - tidyverse\n    - tidymodels\n    - forested\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n<!-- begin: webr fodder -->\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| echo: false\n#| output: false\noptions(width = 60)\nset.seed(20241112)\nforested_split <- initial_split(forested)\nforested_train <- training(forested_split)\nforested_test <- testing(forested_split)\n```\n:::\n\n\n\n\n<!-- end: webr fodder -->\n\n<!-- begin: ae definition -->\n\n\n\n\n\n\n\n\n\n<!-- end: ae definition -->\n\n# Warm-up\n\n## While you wait... {.smaller}\n\n::: appex\n-   Go to your `ae` project in RStudio.\n\n-   Make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   Click Pull to get today's application exercise file: *ae\\-17\\-forest\\-classification\\.qmd*.\n\n-   Wait till the you're prompted to work on the application exercise during class before editing the file.\n:::\n\n## Announcements {.smaller}\n\n-   Project milestone 3 - Improvement and progress due at 5 pm on Friday\n\n# Washington forests\n\n## Packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(forested)\n```\n:::\n\n\n\n\n## Data\n\n-   The U.S. Forest Service maintains machine learning models to predict whether a plot of land is \"forested.\"\n\n-   This classification is important for research, legislation, land management, etc. purposes.\n\n-   Plots are typically remeasured every 10 years.\n\n-   The `forested` dataset contains the most recent measurement per plot.\n\n## Data: `forested`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7,107 √ó 19\n   forested  year elevation eastness northness roughness tree_no_tree\n   <fct>    <dbl>     <dbl>    <dbl>     <dbl>     <dbl> <fct>       \n 1 Yes       2005       881       90        43        63 Tree        \n 2 Yes       2005       113      -25        96        30 Tree        \n 3 No        2005       164      -84        53        13 Tree        \n 4 Yes       2005       299       93        34         6 No tree     \n 5 Yes       2005       806       47       -88        35 Tree        \n 6 Yes       2005       736      -27       -96        53 Tree        \n 7 Yes       2005       636      -48        87         3 No tree     \n 8 Yes       2005       224      -65       -75         9 Tree        \n 9 Yes       2005        52      -62        78        42 Tree        \n10 Yes       2005      2240      -67       -74        99 No tree     \n# ‚Ñπ 7,097 more rows\n# ‚Ñπ 12 more variables: dew_temp <dbl>, precip_annual <dbl>,\n#   temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#   vapor_max <dbl>, canopy_cover <dbl>, lon <dbl>, lat <dbl>,\n#   land_type <fct>\n```\n\n\n:::\n:::\n\n\n\n\n## Data: `forested`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(forested)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 7,107\nColumns: 19\n$ forested         <fct> Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes,‚Ä¶\n$ year             <dbl> 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2‚Ä¶\n$ elevation        <dbl> 881, 113, 164, 299, 806, 736, 636, 224, 52,‚Ä¶\n$ eastness         <dbl> 90, -25, -84, 93, 47, -27, -48, -65, -62, -‚Ä¶\n$ northness        <dbl> 43, 96, 53, 34, -88, -96, 87, -75, 78, -74,‚Ä¶\n$ roughness        <dbl> 63, 30, 13, 6, 35, 53, 3, 9, 42, 99, 51, 19‚Ä¶\n$ tree_no_tree     <fct> Tree, Tree, Tree, No tree, Tree, Tree, No t‚Ä¶\n$ dew_temp         <dbl> 0.04, 6.40, 6.06, 4.43, 1.06, 1.35, 1.42, 6‚Ä¶\n$ precip_annual    <dbl> 466, 1710, 1297, 2545, 609, 539, 702, 1195,‚Ä¶\n$ temp_annual_mean <dbl> 6.42, 10.64, 10.07, 9.86, 7.72, 7.89, 7.61,‚Ä¶\n$ temp_annual_min  <dbl> -8.32, 1.40, 0.19, -1.20, -5.98, -6.00, -5.‚Ä¶\n$ temp_annual_max  <dbl> 12.91, 15.84, 14.42, 15.78, 13.84, 14.66, 1‚Ä¶\n$ temp_january_min <dbl> -0.08, 5.44, 5.72, 3.95, 1.60, 1.12, 0.99, ‚Ä¶\n$ vapor_min        <dbl> 78, 34, 49, 67, 114, 67, 67, 31, 60, 79, 17‚Ä¶\n$ vapor_max        <dbl> 1194, 938, 754, 1164, 1254, 1331, 1275, 944‚Ä¶\n$ canopy_cover     <dbl> 50, 79, 47, 42, 59, 36, 14, 27, 82, 12, 74,‚Ä¶\n$ lon              <dbl> -118.6865, -123.0825, -122.3468, -121.9144,‚Ä¶\n$ lat              <dbl> 48.69537, 47.07991, 48.77132, 45.80776, 48.‚Ä¶\n$ land_type        <fct> Tree, Tree, Tree, Tree, Tree, Tree, Non-tre‚Ä¶\n```\n\n\n:::\n:::\n\n\n\n\n## Outcome and predictors {.smaller}\n\n-   Outcome: `forested` - Factor, `Yes` or `No`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels(forested$forested)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Yes\" \"No\" \n```\n\n\n:::\n:::\n\n\n\n\n-   Predictors: 18 remotely-sensed and easily-accessible predictors:\n\n    -   numeric variables based on weather and topography\n\n    -   categorical variables based on classifications from other governmental organizations\n\n## `?forested`\n\n<iframe width=\"900\" height=\"500\" src=\"https://simonpcouch.github.io/forested/reference/forested.html\" title=\"forested\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## Should we include a predictor?\n\nTo determine whether we should include a predictor in a model, we should start by asking:\n\n::: incremental\n-   Is it ethical to use this variable?\n    (Or even legal?)\n\n-   Will this variable be available at prediction time?\n\n-   Does this variable contribute to explainability?\n:::\n\n# Data splitting and spending\n\n## We've been cheating!\n\n::: incremental\n-   So far, we've been using all the data we have for building models.\n    In predictive contexts, this would be considered *cheating*.\n\n-   Evaluating model performance for predicting outcomes that were used when building the models is like evaluating your learning with questions whose answers you've already seen.\n:::\n\n## Spending your data {.smaller}\n\nFor predictive models (used primarily in machine learning), we typically split data into training and test sets:\n\n![](images/20/test-train-split-1.svg){fig-align=\"center\"}\n\n-   The **training set** is used to estimate model parameters.\n\n-   The **test set** is used to find an independent assessment of model performance.\n\n. . .\n\n::: callout-warning\nDo not use, or even peek at, the test set during training.\n:::\n\n## How much to spend?\n\n::: incremental\n-   The more data we spend (use in training), the better estimates we‚Äôll get.\n\n-   Spending too much data in training prevents us from computing a good assessment of predictive performance.\n\n-   Spending too much data in testing prevents us from computing a good estimate of model parameters.\n:::\n\n## The initial split\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20241112)\nforested_split <- initial_split(forested)\nforested_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<5330/1777/7107>\n```\n\n\n:::\n:::\n\n\n\n\n## Setting a seed {.smaller}\n\n::: task\nWhat does `set.seed()` do?\n:::\n\n::: incremental\n-   To create that split of the data, R generates ‚Äúpseudo-random‚Äù numbers: while they are made to behave like random numbers, their generation is deterministic given a ‚Äúseed‚Äù.\n\n-   This allows us to reproduce results by setting that seed.\n\n-   Which seed you pick doesn‚Äôt matter, as long as you don‚Äôt try a bunch of seeds and pick the one that gives you the best performance.\n:::\n\n## Accessing the data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_train <- training(forested_split)\nforested_test <- testing(forested_split)\n```\n:::\n\n\n\n\n## The training set\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,330 √ó 19\n   forested  year elevation eastness northness roughness tree_no_tree\n   <fct>    <dbl>     <dbl>    <dbl>     <dbl>     <dbl> <fct>       \n 1 Yes       2013       315      -17        98        92 Tree        \n 2 No        2018       374       93       -34        23 No tree     \n 3 No        2017       377       44       -89         1 Tree        \n 4 Yes       2013       541       31       -94       139 Tree        \n 5 Yes       2017       680       14       -98        20 Tree        \n 6 Yes       2017      1482       76       -64        43 Tree        \n 7 No        2020        84       42       -90        12 No tree     \n 8 Yes       2011       210       34        93        16 Tree        \n 9 No        2020       766       14        98        20 No tree     \n10 Yes       2013      1559       98        16        79 Tree        \n# ‚Ñπ 5,320 more rows\n# ‚Ñπ 12 more variables: dew_temp <dbl>, precip_annual <dbl>,\n#   temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#   vapor_max <dbl>, canopy_cover <dbl>, lon <dbl>, lat <dbl>,\n#   land_type <fct>\n```\n\n\n:::\n:::\n\n\n\n\n## The testing data\n\n. . .\n\nüôà\n\n# Exploratory data analysis\n\n## Initial questions {.smaller}\n\n-   What‚Äôs the distribution of the outcome, `forested`?\n\n-   What‚Äôs the distribution of numeric variables like `precip_annual`?\n\n-   How does the distribution of forested differ across the categorical and numerical variables?\n\n. . .\n\n::: task\nWhich dataset should we use for the exploration?\nThe entire data `forested`, the training data `forested_train`, or the testing data `forested_test`?\n:::\n\n## `forested`\n\nWhat‚Äôs the distribution of the outcome, `forested`?\n\n\n\n\n::: {.cell}\n```{webr}\n# add code here\n```\n:::\n\n\n\n\n## `precip_annual`\n\nWhat‚Äôs the distribution of `precip_annual`?\n\n\n\n\n::: {.cell}\n```{webr}\n# add code here\n```\n:::\n\n\n\n\n## `forested` and `precip_annual` {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  forested_train,\n  aes(x = precip_annual, fill = forested, group = forested)\n  ) +\n  geom_histogram(binwidth = 200, position = \"identity\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](20-evaluating-models_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n\n## `forested` and `precip_annual` {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|5\"}\nggplot(\n  forested_train,\n  aes(x = precip_annual, fill = forested, group = forested)\n  ) +\n  geom_histogram(binwidth = 200, position = \"fill\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](20-evaluating-models_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n## `forested` and `tree_no_tree` {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_train, aes(x = tree_no_tree, fill = forested)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](20-evaluating-models_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n\n\n## `forested` and `lat` / `lon` {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(forested_train, aes(x = lon, y = lat, color = forested)) +\n  geom_point(alpha = 0.7) +\n  scale_color_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](20-evaluating-models_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n\n# Terminology\n\n## False negative and positive\n\n-   **False negative rate** is the proportion of actual positives that were classified as negatives.\n\n-   **False positive rate** is the proportion of actual negatives that were classified as positives.\n\n## Sensitivity\n\n**Sensitivity** is the proportion of actual positives that were correctly classified as positive.\n\n-   Also known as **true positive rate** and **recall**\n\n-   Sensitivity = 1 ‚àí False negative rate\n\n-   Useful when false negatives are more \"expensive\" than false positives\n\n## Specificity\n\n**Specificity** is the proportion of actual negatives that were correctly classified as negative\n\n-   Also known as **true negative rate**\n\n-   Specificity = 1 ‚àí False positive rate\n\n## ROC curve\n\nThe **receiver operating characteristic (ROC) curve** allows to assess the model performance across a range of thresholds.\n\n![](images/20/roc-curve.png)\n\n## ROC curve {.smaller}\n\n::: task\nWhich corner of the plot indicates the best model performance?\n:::\n\n![](images/20/roc-curve-annotated.png)\n\n# Next steps\n\n## Next steps {.smaller}\n\n::: incremental\n-   Fit models on training data\n\n-   Make predictions on testing data\n\n-   Evaluate predictions on testing data:\n\n    -   Linear models: R-squared, adjusted R-squared, RMSE (root mean squared error), etc.\n    -   Logistic models: False negative and positive rates, AUC (area under the curve), etc.\n\n-   Make decisions based on model predictive performance, validity across various testing/training splits (aka \"cross validation\"), explainability\n:::\n\n. . .\n\n::: callout-note\nWe will only learn about a subset of these in this course, but you can go further into these ideas in STA 210 or STA 221 as well as in various machine learning courses.\n:::\n\n## ae\\-17\\-forest\\-classification {.smaller}\n\n::: appex\n-   Go to your ae project in RStudio.\n\n-   If you haven't yet done so, make sure all of your changes up to this point are committed and pushed, i.e., there's nothing left in your Git pane.\n\n-   If you haven't yet done so, click Pull to get today's application exercise file: *ae\\-17\\-forest\\-classification\\.qmd*. You might be prompted to install **forested**, say yes.\n\n-   Work through the application exercise in class, and render, commit, and push your edits.\n:::\n",
    "supporting": [
      "20-evaluating-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}