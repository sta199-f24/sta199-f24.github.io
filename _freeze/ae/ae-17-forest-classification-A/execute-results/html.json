{
  "hash": "ef2260eeb7218a7e50a9b2dbe79a5515",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AE 17: Forest classification\"\nsubtitle: \"Suggested answers\"\ncategories: \n  - Application exercise\n  - Answers\n---\n\n\n\n::: callout-important\nThese are suggested answers.\nThis document should be used as reference only, it's not designed to be an exhaustive key.\n:::\n\nIn this application exercise, we will\n\n-   Split our data into testing and training\n-   Fit logistic regression regression models to testing data to classify outcomes\n-   Evaluate performance of models on testing data\n\nWe will use **tidyverse** and **tidymodels** for data exploration and modeling, respectively, and the **forested** package for the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(forested)\n```\n:::\n\n\n\nRemember from the lecture that the `forested` dataset contains information on whether a plot is forested (`Yes`) or not (`No`) as well as numerical and categorical features of that plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(forested)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 7,107\nColumns: 19\n$ forested         <fct> Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes,…\n$ year             <dbl> 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005,…\n$ elevation        <dbl> 881, 113, 164, 299, 806, 736, 636, 224, 52, 2240, 104…\n$ eastness         <dbl> 90, -25, -84, 93, 47, -27, -48, -65, -62, -67, 96, -4…\n$ northness        <dbl> 43, 96, 53, 34, -88, -96, 87, -75, 78, -74, -26, 86, …\n$ roughness        <dbl> 63, 30, 13, 6, 35, 53, 3, 9, 42, 99, 51, 190, 95, 212…\n$ tree_no_tree     <fct> Tree, Tree, Tree, No tree, Tree, Tree, No tree, Tree,…\n$ dew_temp         <dbl> 0.04, 6.40, 6.06, 4.43, 1.06, 1.35, 1.42, 6.39, 6.50,…\n$ precip_annual    <dbl> 466, 1710, 1297, 2545, 609, 539, 702, 1195, 1312, 103…\n$ temp_annual_mean <dbl> 6.42, 10.64, 10.07, 9.86, 7.72, 7.89, 7.61, 10.45, 10…\n$ temp_annual_min  <dbl> -8.32, 1.40, 0.19, -1.20, -5.98, -6.00, -5.76, 1.11, …\n$ temp_annual_max  <dbl> 12.91, 15.84, 14.42, 15.78, 13.84, 14.66, 14.23, 15.3…\n$ temp_january_min <dbl> -0.08, 5.44, 5.72, 3.95, 1.60, 1.12, 0.99, 5.54, 6.20…\n$ vapor_min        <dbl> 78, 34, 49, 67, 114, 67, 67, 31, 60, 79, 172, 162, 70…\n$ vapor_max        <dbl> 1194, 938, 754, 1164, 1254, 1331, 1275, 944, 892, 549…\n$ canopy_cover     <dbl> 50, 79, 47, 42, 59, 36, 14, 27, 82, 12, 74, 66, 83, 6…\n$ lon              <dbl> -118.6865, -123.0825, -122.3468, -121.9144, -117.8841…\n$ lat              <dbl> 48.69537, 47.07991, 48.77132, 45.80776, 48.07396, 48.…\n$ land_type        <fct> Tree, Tree, Tree, Tree, Tree, Tree, Non-tree vegetati…\n```\n\n\n:::\n:::\n\n\n\n# Spending your data\n\nSplit your data into testing and training in a reproducible manner and display the split object.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nforested_split <- initial_split(forested)\nforested_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<5330/1777/7107>\n```\n\n\n:::\n:::\n\n\n\nWhat percent of the original `forested` data is allocated to training and what percent to testing?\nCompare your response to your neighbor's.\nAre the percentages roughly consistent?\nWhat determines this in the `initial_split()`?\nHow would the code need to be updated to allocate 80% of the data to training and the remaining 20% to testing?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# training percentage\n5330 / 7107\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7499648\n```\n\n\n:::\n\n```{.r .cell-code}\n# testing percentage\n1770 / 7107\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2490502\n```\n\n\n:::\n:::\n\n\n\nRoughly 75% of the data is allocated to training and the remaining 25% to testing.\nThis is because the `prop` argument in `initial_split()` is `3/4` by default.\nThe code would need to be updated as follows for a 80%/20% split:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# split 80-20\nset.seed(1234)\ninitial_split(forested, prop = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<5685/1422/7107>\n```\n\n\n:::\n\n```{.r .cell-code}\n# training percentage\n5685 / 7107\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7999156\n```\n\n\n:::\n\n```{.r .cell-code}\n# testing percentage\n1422 / 7107\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2000844\n```\n\n\n:::\n:::\n\n\n\nLet's stick with the default split and save our testing and training data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_train <- training(forested_split)\nforested_test <- testing(forested_split)\n```\n:::\n\n\n\n# Exploratory data analysis\n\nCreate a visualization that explores the relationship between the outcome, one numerical predictor, and one categorical predictor.\nThen, describe, in a few sentences, what the visualization shows about the relationship between these variables.\n\n**Note:** Pay attention to which dataset you use for your exploration.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(\n  forested_train,\n  aes(x = temp_annual_mean, fill = forested, group = forested)\n  ) +\n  geom_histogram(position = \"identity\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"Yes\" = \"forestgreen\", \"No\" = \"gold2\")) +\n  facet_wrap(~land_type, ncol = 1) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ae-17-forest-classification-A_files/figure-html/eda-plot-1.png){width=672}\n:::\n:::\n\n\n\nRegardless of land type, typical mean annual temperatures in non-forested areas are higher than typical mean annual temperatures in forested areas.\nThe shape of the distribution of mean annual temperatures does not seem to vary by land type or forestation.\n\n# Model 1: Custom choice of predictors\n\n## Fit\n\nFit a model for classifying plots as forested or not based on a subset of predictors of your choice.\nName the model `forested_custom_fit` and display a tidy output of the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_fit <- logistic_reg() |>\n  fit(forested ~ elevation + tree_no_tree + lat + lon + temp_annual_mean, data = forested_train)\ntidy(forested_custom_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  term                 estimate std.error statistic   p.value\n  <chr>                   <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)         48.2       6.06         7.96  1.71e- 15\n2 elevation           -0.000311  0.000384    -0.811 4.17e-  1\n3 tree_no_treeNo tree  3.35      0.0925      36.3   4.02e-288\n4 lat                 -0.313     0.0642      -4.87  1.09e-  6\n5 lon                  0.311     0.0288      10.8   3.81e- 27\n6 temp_annual_mean     0.270     0.0819       3.30  9.78e-  4\n```\n\n\n:::\n:::\n\n\n\n## Predict\n\nPredict for the testing data using this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_aug <- augment(forested_custom_fit, new_data = forested_test)\nforested_custom_aug\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,777 × 22\n   .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n   <fct>           <dbl>    <dbl> <fct>    <dbl>     <dbl>    <dbl>     <dbl>\n 1 Yes            0.877    0.123  Yes       2005       113      -25        96\n 2 Yes            0.829    0.171  Yes       2005       736      -27       -96\n 3 Yes            0.855    0.145  Yes       2005       224      -65       -75\n 4 Yes            0.957    0.0431 Yes       2003      1031      -49        86\n 5 Yes            0.728    0.272  No        2005      1713      -66        75\n 6 Yes            0.982    0.0175 Yes       2014      1612       30       -95\n 7 No             0.0765   0.924  No        2014       507       44       -89\n 8 Yes            0.889    0.111  Yes       2014       940      -93        35\n 9 Yes            0.659    0.341  No        2014       246       22       -97\n10 No             0.0877   0.912  No        2014       419       86       -49\n# ℹ 1,767 more rows\n# ℹ 14 more variables: roughness <dbl>, tree_no_tree <fct>, dew_temp <dbl>,\n#   precip_annual <dbl>, temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#   vapor_max <dbl>, canopy_cover <dbl>, lon <dbl>, lat <dbl>, land_type <fct>\n```\n\n\n:::\n:::\n\n\n\n## Evaluate\n\nCalculate the false positive and false negative rates for the testing data using this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_aug |>\n  count(.pred_class, forested) |>\n  arrange(forested) |>\n  group_by(forested) |>\n  mutate(\n    p = round(n / sum(n), 2),\n    decision = case_when(\n      .pred_class == \"Yes\" & forested == \"Yes\" ~ \"True positive\",\n      .pred_class == \"Yes\" & forested == \"No\" ~ \"False positive\",\n      .pred_class == \"No\" & forested == \"Yes\" ~ \"False negative\",\n      .pred_class == \"No\" & forested == \"No\" ~ \"True negative\"\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n# Groups:   forested [2]\n  .pred_class forested     n     p decision      \n  <fct>       <fct>    <int> <dbl> <chr>         \n1 Yes         Yes        864  0.9  True positive \n2 No          Yes         98  0.1  False negative\n3 Yes         No         124  0.15 False positive\n4 No          No         691  0.85 True negative \n```\n\n\n:::\n:::\n\n\n\nAnother commonly used display of this information is a confusion matrix.\nCreate this using the `conf_mat()` function.\nYou will need to review the documentation for the function to determine how to use it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mat(\n  forested_custom_aug, \n  truth = forested, \n  estimate = .pred_class\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction Yes  No\n       Yes 864 124\n       No   98 691\n```\n\n\n:::\n:::\n\n\n\n## Sensitivity, specificity, ROC curve\n\nCalculate sensitivity and specificity and draw the ROC curve.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_roc <- roc_curve(forested_custom_aug, truth = forested, .pred_Yes)\nforested_custom_roc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,779 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1  -Inf          0                 1\n 2     0.0186     0                 1\n 3     0.0202     0.00123           1\n 4     0.0204     0.00245           1\n 5     0.0207     0.00368           1\n 6     0.0227     0.00491           1\n 7     0.0286     0.00613           1\n 8     0.0298     0.00736           1\n 9     0.0299     0.00859           1\n10     0.0300     0.00982           1\n# ℹ 1,769 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(forested_custom_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](ae-17-forest-classification-A_files/figure-html/forested-custom-roc-1.png){width=672}\n:::\n:::\n\n\n\n# Model 2: All predictors\n\n## Fit\n\nFit a model for classifying plots as forested or not based on all predictors available.\nName the model `forested_full_fit` and display a tidy output of the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_fit <- logistic_reg() |>\n  fit(forested ~ ., data = forested_train)\ntidy(forested_full_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 20 × 5\n   term                             estimate std.error statistic  p.value\n   <chr>                               <dbl>     <dbl>     <dbl>    <dbl>\n 1 (Intercept)                  -12.1        32.6       -0.371   7.11e- 1\n 2 year                           0.00456     0.0152     0.299   7.65e- 1\n 3 elevation                     -0.00277     0.000639  -4.33    1.47e- 5\n 4 eastness                      -0.000910    0.000734  -1.24    2.15e- 1\n 5 northness                      0.00208     0.000745   2.79    5.26e- 3\n 6 roughness                     -0.00399     0.00146   -2.73    6.29e- 3\n 7 tree_no_treeNo tree            1.25        0.136      9.23    2.61e-20\n 8 dew_temp                      -0.125       0.176     -0.712   4.76e- 1\n 9 precip_annual                 -0.0000895   0.000100  -0.895   3.71e- 1\n10 temp_annual_mean              -7.30       12.4       -0.587   5.57e- 1\n11 temp_annual_min                0.819       0.103      7.93    2.20e-15\n12 temp_annual_max                2.59        6.22       0.417   6.77e- 1\n13 temp_january_min               3.34        6.21       0.538   5.91e- 1\n14 vapor_min                      0.00000990  0.00353    0.00280 9.98e- 1\n15 vapor_max                      0.00925     0.00132    7.00    2.62e-12\n16 canopy_cover                  -0.0446      0.00366  -12.2     4.18e-34\n17 lon                           -0.0953      0.0559    -1.71    8.80e- 2\n18 lat                            0.0748      0.109      0.683   4.94e- 1\n19 land_typeNon-tree vegetation  -0.735       0.282     -2.61    9.05e- 3\n20 land_typeTree                 -1.58        0.297     -5.33    9.93e- 8\n```\n\n\n:::\n:::\n\n\n\n## Predict\n\nPredict for the testing data using this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_aug <- augment(forested_full_fit, new_data = forested_test)\nforested_full_aug\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,777 × 22\n   .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n   <fct>           <dbl>    <dbl> <fct>    <dbl>     <dbl>    <dbl>     <dbl>\n 1 Yes            0.930    0.0700 Yes       2005       113      -25        96\n 2 Yes            0.918    0.0822 Yes       2005       736      -27       -96\n 3 No             0.428    0.572  Yes       2005       224      -65       -75\n 4 Yes            0.972    0.0280 Yes       2003      1031      -49        86\n 5 Yes            0.633    0.367  No        2005      1713      -66        75\n 6 Yes            0.980    0.0201 Yes       2014      1612       30       -95\n 7 No             0.0436   0.956  No        2014       507       44       -89\n 8 Yes            0.845    0.155  Yes       2014       940      -93        35\n 9 No             0.0141   0.986  No        2014       246       22       -97\n10 No             0.0386   0.961  No        2014       419       86       -49\n# ℹ 1,767 more rows\n# ℹ 14 more variables: roughness <dbl>, tree_no_tree <fct>, dew_temp <dbl>,\n#   precip_annual <dbl>, temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#   vapor_max <dbl>, canopy_cover <dbl>, lon <dbl>, lat <dbl>, land_type <fct>\n```\n\n\n:::\n:::\n\n\n\n## Evaluate\n\nCalculate the false positive and false negative rates for the testing data using this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_aug |>\n  count(.pred_class, forested) |>\n  arrange(forested) |>\n  group_by(forested) |>\n  mutate(\n    p = round(n / sum(n), 2),\n    decision = case_when(\n      .pred_class == \"Yes\" & forested == \"Yes\" ~ \"True positive\",\n      .pred_class == \"Yes\" & forested == \"No\" ~ \"False positive\",\n      .pred_class == \"No\" & forested == \"Yes\" ~ \"False negative\",\n      .pred_class == \"No\" & forested == \"No\" ~ \"True negative\"\n    )\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n# Groups:   forested [2]\n  .pred_class forested     n     p decision      \n  <fct>       <fct>    <int> <dbl> <chr>         \n1 Yes         Yes        876  0.91 True positive \n2 No          Yes         86  0.09 False negative\n3 Yes         No          85  0.1  False positive\n4 No          No         730  0.9  True negative \n```\n\n\n:::\n:::\n\n\n\n## Sensitivity, specificity, ROC curve\n\nCalculate sensitivity and specificity and draw the ROC curve.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_full_roc <- roc_curve(forested_full_aug, truth = forested, .pred_Yes)\nforested_full_roc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,779 × 3\n   .threshold specificity sensitivity\n        <dbl>       <dbl>       <dbl>\n 1 -Inf           0                 1\n 2    0.00106     0                 1\n 3    0.00107     0.00123           1\n 4    0.00217     0.00245           1\n 5    0.00287     0.00368           1\n 6    0.00290     0.00491           1\n 7    0.00290     0.00613           1\n 8    0.00309     0.00736           1\n 9    0.00321     0.00859           1\n10    0.00323     0.00982           1\n# ℹ 1,769 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(forested_full_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](ae-17-forest-classification-A_files/figure-html/forested-full-roc-1.png){width=672}\n:::\n:::\n\n\n\n# Model 1 vs. Model 2\n\nPlot both ROC curves and articulate how you would use them to compare these models.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_custom_roc <- forested_custom_roc |>\n  mutate(model = \"Custom\")\nforested_full_roc <- forested_full_roc |>\n  mutate(model = \"Full\")\nbind_rows(\n  forested_custom_roc,\n  forested_full_roc\n) |>\n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_path() +\n  geom_abline(lty = 3) +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](ae-17-forest-classification-A_files/figure-html/compare-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "ae-17-forest-classification-A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}